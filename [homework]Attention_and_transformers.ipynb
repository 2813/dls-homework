{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2813/dls-homework/blob/main/%5Bhomework%5DAttention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\" width=\"400\"></p>\n",
        "\n",
        "# Глубокое обучение. Часть 2\n",
        "# Домашнее задание по теме \"Механизм внимания\""
      ],
      "metadata": {
        "id": "Ji8KtYOVGs8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это домашнее задание проходит в формате peer-review. Это означает, что его будут проверять ваши однокурсники. Поэтому пишите разборчивый код, добавляйте комментарии и пишите выводы после проделанной работы.\n",
        "\n",
        "В этом задании вы будете решать задачу классификации математических задач по темам (многоклассовая классификация) с помощью Transformer.\n",
        "\n",
        "В качестве датасета возьмем датасет математических задач по разным темам. Нам необходим следующий файл:\n",
        "\n",
        "[Файл с классами](https://docs.google.com/spreadsheets/d/1IMRxByfg7gjoZ5i7rxvuNDvSrbdOJOc-/edit?usp=drive_link&ouid=104379615679964018037&rtpof=true&sd=true)"
      ],
      "metadata": {
        "id": "UAr-M8_1GJ6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint:** не перезаписывайте модели, которые вы получите на каждом из этапов этого дз. Они ещё понадобятся."
      ],
      "metadata": {
        "id": "1fybMcmV0YRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1 (2 балла)\n",
        "\n",
        "Напишите кастомный класс для модели трансформера для задачи классификации, использующей в качествке backbone какую-то из моделей huggingface.\n",
        "\n",
        "Т.е. конструктор класса должен принимать на вход название модели и подгружать её из huggingface, а затем использовать в качестве backbone (достаточно возможности использовать в качестве backbone те модели, которые упомянуты в последующих пунктах)"
      ],
      "metadata": {
        "id": "t395freCxpOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification, Trainer, TrainerArguments, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from transformers.modeling_outputs import TokenClassifierOutput"
      ],
      "metadata": {
        "id": "wmd3kZnnbUHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "4jk_vJFVcdB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### This is just an interface example. You may change it if you want.\n",
        "\n",
        "class TransformerClassificationModel(nn.Module):\n",
        "    def __init__(base_transformer_model: Union[str, nn.Module], num_classes: int=7):\n",
        "        self.backbone = AutoModelForSequenceClassification.from_pretrained(base_transformer_model)\n",
        "\n",
        "\n",
        "        # YOUR CODE: create additional layers for classfication\n",
        "        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(inputs, ...):\n",
        "        # YOUR CODE: propagate inputs through the model. Return dict with logits\n",
        "        outputs = self.backbone(input_ids, attention_mask)['last_hidden_state'][0][0,:]\n",
        "        logits = self.classifier(outputs)\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=nn.CrossEntropyLoss(),\n",
        "            logits=self.classifier(outputs)\n",
        "            hidden_states=outputs,\n",
        "            attentions=outputs.attentions\n",
        "            )"
      ],
      "metadata": {
        "id": "eX4VGWquyiMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2 (1 балл)\n",
        "\n",
        "Напишите функцию заморозки backbone у модели (если необходимо, возвращайте из функции модель)"
      ],
      "metadata": {
        "id": "Vd3kxX6hy0d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_backbone_function(model: TransformerClassificationModel):\n",
        "    for p in model.backbone.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "    for p in model.classifier.parameters():\n",
        "      p.requires_grad = True"
      ],
      "metadata": {
        "id": "U8IuDosbzKe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3 (2 балла)\n",
        "\n",
        "Напишите функцию, которая будет использована для тренировки (дообучения) трансформера (TransformerClassificationModel). Функция должна поддерживать обучение с замороженным и размороженным backbone."
      ],
      "metadata": {
        "id": "kybkw6MSzd-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def train_transformer(transformer_model, dataloader, freeze_backbone=True)\n",
        "    model = copy.deepcopy(transformer_model)\n",
        "    ### YOUR CODE IS HERE\n",
        "    if freeze_backbone:\n",
        "      freeze_backbone_funtions(model)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    data_collator = transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "    training_args = transformers.Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=3,\n",
        "        num_train_epochs=2,\n",
        "    )\n",
        "    trainer = transformers.Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_loader,\n",
        "        eval_dataset=test_loader,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EDhrD0BHzxi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4 (1 балл)\n",
        "\n",
        "Проверьте вашу функцию из предыдущего пункта, дообучив двумя способами\n",
        "*cointegrated/rubert-tiny2* из huggingface."
      ],
      "metadata": {
        "id": "eUqhI4mV_RTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rubert_tiny_transformer_model = TransformerClassificationModel(AutoModel.from_pretrained('cointegrated/rubert-tiny2'), num_classes=7)\n",
        "rubert_tiny_finetuned_with_freezed_backbone = train_transformer(rubert_tiny_transformer_model, train_loader freeze_backbone=True)\n",
        "\n",
        "rubert_tiny_transformer_model = TransformerClassificationModel(AutoModel.from_pretrained('cointegrated/rubert-tiny2'), num_classes=7)\n",
        "rubert_tiny_full_finetuned = train_transformer(rubert_tiny_transformer_model, train_loader, freeze_backbone=False)"
      ],
      "metadata": {
        "id": "nuxOCBQHAKZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 5 (1 балл)\n",
        "\n",
        "Обучите *tbs17/MathBert* (с замороженным backbone и без заморозки), проанализируйте результаты. Сравните скоры с первым заданием. Получилось лучше или нет? Почему?"
      ],
      "metadata": {
        "id": "zRi7tkoOAjon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE (probably, similar on the previous step)\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('tbs17/MathBert')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('tbs17/MathBert')\n",
        "\n",
        "mathBert_model = TransformerClassificationModel(model, num_classes=7)\n",
        "mathbert_frozen = train_transformer(mathBert_model, train_loader, freeze_backbone=True)\n",
        "\n",
        "mathbert_unfrozen = train_transformer(mathBert_model, train_loader, freeze_backbone=False)"
      ],
      "metadata": {
        "id": "XKtd3YgNA14E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 6 (1 балл)\n",
        "\n",
        "Напишите функцию для отрисовки карт внимания первого слоя для моделей из задания"
      ],
      "metadata": {
        "id": "EuU6Di26017B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_first_layer_attention_maps(attention_head_ids: List, text: str, model: TransformerClassificationModel):\n",
        "    pass"
      ],
      "metadata": {
        "id": "guzGxfcV1Cba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 7 (1 балл)\n",
        "\n",
        "Проведите инференс для всех моделей **ДО ДООБУЧЕНИЯ** на 2-3 текстах из датасета. Посмотрите на головы Attention первого слоя в каждой модели на выбранных текстах (отрисуйте их отдельно).\n",
        "\n",
        "Попробуйте их проинтерпретировать. Какие связи улавливают карты внимания? (если в модели много голов Attention, то проинтерпретируйте наиболее интересные)"
      ],
      "metadata": {
        "id": "Iu0adKw4BLtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE"
      ],
      "metadata": {
        "id": "U2gEF3vkB6eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 8 (1 балл)\n",
        "\n",
        "Сделайте то же самое для дообученных моделей. Изменились ли карты внимания и связи, которые они улавливают? Почему?"
      ],
      "metadata": {
        "id": "pBNVrOpCCLqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE"
      ],
      "metadata": {
        "id": "F5229WBICWEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}